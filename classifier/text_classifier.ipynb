{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhairya/projects/sih/ml_components/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to /home/dhairya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2023-09-18 20:37:39.517008: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-18 20:37:39.560660: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-18 20:37:40.329792: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.0.1+cu117\n",
      "Device: NVIDIA GeForce MX350\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from statistics import median\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer, PorterStemmer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "from datasets import Dataset\n",
    "import evaluate\n",
    "\n",
    "print(f'Pytorch version: {torch.__version__}')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    print(f'Device: {torch.cuda.get_device_name(torch.cuda.current_device())}')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "MAX_TOKENS = 16384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F. NARIMAN, J. Leave granted. In 2008, the Pu...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>2019_890.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S. THAKUR, J. Leave granted. These appeals ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2014_170.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Markandey Katju, J. Leave granted. Heard lear...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>2010_721.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALTAMAS KABIR,J. Leave granted. The question ...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>2008_1460.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CIVIL APPEAL NO. 598 OF 2007 K. MATHUR, J. Th...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>2008_188.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  split  \\\n",
       "0   F. NARIMAN, J. Leave granted. In 2008, the Pu...      1  train   \n",
       "1   S. THAKUR, J. Leave granted. These appeals ar...      0  train   \n",
       "2   Markandey Katju, J. Leave granted. Heard lear...      1  train   \n",
       "3   ALTAMAS KABIR,J. Leave granted. The question ...      1  train   \n",
       "4   CIVIL APPEAL NO. 598 OF 2007 K. MATHUR, J. Th...      1  train   \n",
       "\n",
       "            name  \n",
       "0   2019_890.txt  \n",
       "1   2014_170.txt  \n",
       "2   2010_721.txt  \n",
       "3  2008_1460.txt  \n",
       "4   2008_188.txt  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/ILDC_single/ILDC_single.csv/ILDC_single.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.67, 0.2, 0.13]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data split percentages\n",
    "data_s = Counter(data['split'])\n",
    "num_rows = sum(data_s.values())\n",
    "[float(f'{val/num_rows:.2f}') for val in data_s.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = data[data['split'] == 'train'].drop(['split', 'name'], axis=1)\n",
    "dev_ds = data[data['split'] == 'dev'].drop(['split', 'name'], axis=1)\n",
    "test_ds = data[data['split'] == 'test'].drop(['split', 'name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23650, 521891, 880, 17529)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_ds.loc[:, 'text']\n",
    "lens = ([len(k) for k in x])\n",
    "int(sum(lens)/len(lens)), max(lens), min(lens), int(median(lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 3147, 1: 1935})\n",
      "Counter({0: 497, 1: 497})\n",
      "Counter({1: 762, 0: 755})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(train_ds['label']))\n",
    "print(Counter(dev_ds['label']))\n",
    "print(Counter(test_ds['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label', '__index_level_0__'],\n",
       "     num_rows: 5082\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label', '__index_level_0__'],\n",
       "     num_rows: 994\n",
       " }))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert pd to hugging face datsets\n",
    "hg_train_ds = Dataset.from_pandas(train_ds)\n",
    "hg_dev_ds = Dataset.from_pandas(dev_ds)\n",
    "\n",
    "hg_train_ds, hg_dev_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('hyperonym/xlm-roberta-longformer-base-16384')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown token: <unk>, ID: 3\n",
      "Seperator token: </s>, ID: 2\n",
      "Padding token: <pad>, ID: 1\n",
      "mask token: <mask>, ID: 250001\n",
      "Sentence level classification token: <s>, ID: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Unknown token: {tokenizer.unk_token}, ID: {tokenizer.unk_token_id}')\n",
    "print(f'Seperator token: {tokenizer.sep_token}, ID: {tokenizer.sep_token_id}')\n",
    "print(f'Padding token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}')\n",
    "print(f'mask token: {tokenizer.mask_token}, ID: {tokenizer.mask_token_id}')\n",
    "print(f'Sentence level classification token: {tokenizer.cls_token}, ID: {tokenizer.cls_token_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def preprocess_text(data: Dataset):\n",
    "    text: str = data['text'].strip()\n",
    "    text = text.lower()\n",
    "\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    text = lemmatizer.lemmatize(text, 'v')\n",
    "    text = porter.stem(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_ds(data: Dataset):\n",
    "    text = preprocess_text(data)\n",
    "    tokenized_ds = {\n",
    "        'input_ids': tokenizer.encode(\n",
    "            text, padding='max_length', max_length=MAX_TOKENS, return_tensors=\"pt\"\n",
    "    )}\n",
    "    return tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5082/5082 [01:57<00:00, 43.09 examples/s]\n",
      "Map: 100%|██████████| 994/994 [00:22<00:00, 43.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_a = hg_train_ds.map(tokenize_ds)\n",
    "tokenized_dev_a = hg_dev_ds.map(tokenize_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label', '__index_level_0__', 'input_ids'],\n",
       "     num_rows: 5082\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label', '__index_level_0__', 'input_ids'],\n",
       "     num_rows: 994\n",
       " }))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_a, tokenized_dev_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384.0\n",
      "Counter({True: 4943, False: 139})\n"
     ]
    }
   ],
   "source": [
    "inp_ids = tokenized_train_a[:]['input_ids'][:]\n",
    "tkn_lens = [len(ids[0]) for ids in inp_ids]\n",
    "print(median(tkn_lens))\n",
    "\n",
    "print(Counter([(tkn <= MAX_TOKENS) for tkn in tkn_lens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384.0\n",
      "Counter({True: 971, False: 23})\n"
     ]
    }
   ],
   "source": [
    "inp_idsd = tokenized_dev_a[:]['input_ids'][:]\n",
    "tkn_lensd = [len(ids[0]) for ids in inp_idsd]\n",
    "print(median(tkn_lensd))\n",
    "\n",
    "print(Counter([(tkn <= MAX_TOKENS) for tkn in tkn_lensd]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(np.arange(len(tkn_lens)), tkn_lens)\n",
    "plt.ylim([0, 20_000])\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 5082/5082 [00:33<00:00, 149.70 examples/s]\n",
      "Filter: 100%|██████████| 994/994 [00:06<00:00, 154.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def filter_func(datapoint):\n",
    "    return len(datapoint['input_ids'][0]) <= MAX_TOKENS\n",
    "\n",
    "filtered_train = tokenized_train_a.filter(filter_func)\n",
    "filtered_dev = tokenized_dev_a.filter(filter_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 3055, 1: 1888})\n",
      "Counter({1: 489, 0: 482})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(filtered_train['label']))\n",
    "print(Counter(filtered_dev['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label', '__index_level_0__', 'input_ids'],\n",
       "     num_rows: 4943\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label', '__index_level_0__', 'input_ids'],\n",
       "     num_rows: 971\n",
       " }))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_train, filtered_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_cols = ('text', '__index_level_0__', 'label')\n",
    "tokenized_train = filtered_train.remove_columns(rm_cols)\n",
    "tokenized_dev = filtered_train.remove_columns(rm_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids'],\n",
       "     num_rows: 4943\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['input_ids'],\n",
       "     num_rows: 4943\n",
       " }))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train, tokenized_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4943\n",
      "16384\n",
      "Counter({True: 4943})\n"
     ]
    }
   ],
   "source": [
    "k = tokenized_train[:]['input_ids']\n",
    "\n",
    "print(len(k))\n",
    "tkn_lensk = [len(ids[0]) for ids in k]\n",
    "print(median(tkn_lensk))\n",
    "\n",
    "print(Counter([(tkn == MAX_TOKENS) for tkn in tkn_lensk]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16384.0\n",
      "Counter({True: 971, False: 23})\n"
     ]
    }
   ],
   "source": [
    "inp_idsd = tokenized_dev_a[:]['input_ids'][:]\n",
    "tkn_lensd = [len(ids[0]) for ids in inp_idsd]\n",
    "print(median(tkn_lensd))\n",
    "\n",
    "print(Counter([(tkn <= MAX_TOKENS) for tkn in tkn_lensd]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remname label to labels as pytorch expects it\n",
    "# tokenized_train = tokenized_train.rename_column('label', 'labels')\n",
    "# tokenized_dev = tokenized_dev.rename_column('label', 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change format\n",
    "tokenized_train.set_format('torch')\n",
    "tokenized_dev.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0, 1238,   24,  ...,    1,    1,    1]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda': torch.cuda.empty_cache()\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_dl = DataLoader(dataset=tokenized_train, shuffle=True, batch_size=BATCH_SIZE)\n",
    "dev_dl = DataLoader(dataset=tokenized_dev, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "TRAIN_STEPS = num_rows * len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == 'cuda': torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 20:46:30.092210: W tensorflow/tsl/framework/bfc_allocator.cc:485] Allocator (GPU_0_bfc) ran out of memory trying to allocate 732.43MiB (rounded to 768006144)requested by op StatelessTruncatedNormalV2\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-09-18 20:46:30.092287: I tensorflow/tsl/framework/bfc_allocator.cc:1039] BFCAllocator dump for GPU_0_bfc\n",
      "2023-09-18 20:46:30.092306: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (256): \tTotal Chunks: 50, Chunks in use: 50. 12.5KiB allocated for chunks. 12.5KiB in use in bin. 1.9KiB client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092319: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (512): \tTotal Chunks: 1, Chunks in use: 0. 512B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092332: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1024): \tTotal Chunks: 4, Chunks in use: 4. 4.2KiB allocated for chunks. 4.2KiB in use in bin. 4.0KiB client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092343: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2048): \tTotal Chunks: 3, Chunks in use: 3. 6.0KiB allocated for chunks. 6.0KiB in use in bin. 6.0KiB client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092353: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092363: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092374: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092383: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092393: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092403: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092412: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092422: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092432: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092442: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092451: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092463: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092490: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092510: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092525: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092544: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (134217728): \tTotal Chunks: 1, Chunks in use: 0. 220.12MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092565: I tensorflow/tsl/framework/bfc_allocator.cc:1046] Bin (268435456): \tTotal Chunks: 2, Chunks in use: 2. 1.43GiB allocated for chunks. 1.43GiB in use in bin. 1.43GiB client-requested in use in bin.\n",
      "2023-09-18 20:46:30.092582: I tensorflow/tsl/framework/bfc_allocator.cc:1062] Bin for 732.43MiB was 256.00MiB, Chunk State: \n",
      "2023-09-18 20:46:30.092598: I tensorflow/tsl/framework/bfc_allocator.cc:1075] Next region of size 1766850560\n",
      "2023-09-18 20:46:30.092636: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36000000 of size 256 next 1\n",
      "2023-09-18 20:46:30.092651: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36000100 of size 1280 next 2\n",
      "2023-09-18 20:46:30.092663: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36000600 of size 256 next 3\n",
      "2023-09-18 20:46:30.092675: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36000700 of size 256 next 4\n",
      "2023-09-18 20:46:30.092688: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36000800 of size 256 next 5\n",
      "2023-09-18 20:46:30.092701: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36000900 of size 256 next 6\n",
      "2023-09-18 20:46:30.092713: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36000a00 of size 256 next 7\n",
      "2023-09-18 20:46:30.092726: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36000b00 of size 256 next 8\n",
      "2023-09-18 20:46:30.092739: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36000c00 of size 256 next 9\n",
      "2023-09-18 20:46:30.092752: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36000d00 of size 256 next 10\n",
      "2023-09-18 20:46:30.092765: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36000e00 of size 256 next 11\n",
      "2023-09-18 20:46:30.092778: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36000f00 of size 256 next 17\n",
      "2023-09-18 20:46:30.092791: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36001000 of size 256 next 13\n",
      "2023-09-18 20:46:30.092803: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36001100 of size 256 next 14\n",
      "2023-09-18 20:46:30.092815: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36001200 of size 256 next 15\n",
      "2023-09-18 20:46:30.092829: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36001300 of size 2048 next 16\n",
      "2023-09-18 20:46:30.092843: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36001b00 of size 256 next 12\n",
      "2023-09-18 20:46:30.092857: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36001c00 of size 256 next 21\n",
      "2023-09-18 20:46:30.092871: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36001d00 of size 256 next 22\n",
      "2023-09-18 20:46:30.092884: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36001e00 of size 256 next 23\n",
      "2023-09-18 20:46:30.092897: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36001f00 of size 256 next 20\n",
      "2023-09-18 20:46:30.092911: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36002000 of size 256 next 18\n",
      "2023-09-18 20:46:30.092925: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36002100 of size 1024 next 19\n",
      "2023-09-18 20:46:30.092939: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d36002500 of size 768006144 next 24\n",
      "2023-09-18 20:46:30.092953: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d63c6fd00 of size 768006144 next 25\n",
      "2023-09-18 20:46:30.092968: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918dd500 of size 256 next 26\n",
      "2023-09-18 20:46:30.092977: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918dd600 of size 256 next 27\n",
      "2023-09-18 20:46:30.092985: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918dd700 of size 256 next 28\n",
      "2023-09-18 20:46:30.092993: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918dd800 of size 256 next 29\n",
      "2023-09-18 20:46:30.093002: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918dd900 of size 256 next 30\n",
      "2023-09-18 20:46:30.093010: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918dda00 of size 256 next 31\n",
      "2023-09-18 20:46:30.093019: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918ddb00 of size 256 next 32\n",
      "2023-09-18 20:46:30.093027: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918ddc00 of size 256 next 33\n",
      "2023-09-18 20:46:30.093035: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918ddd00 of size 256 next 34\n",
      "2023-09-18 20:46:30.093043: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918dde00 of size 256 next 39\n",
      "2023-09-18 20:46:30.093052: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918ddf00 of size 256 next 36\n",
      "2023-09-18 20:46:30.093075: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918de000 of size 256 next 37\n",
      "2023-09-18 20:46:30.093083: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918de100 of size 2048 next 38\n",
      "2023-09-18 20:46:30.093091: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918de900 of size 256 next 35\n",
      "2023-09-18 20:46:30.093101: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918dea00 of size 256 next 41\n",
      "2023-09-18 20:46:30.093109: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918deb00 of size 256 next 43\n",
      "2023-09-18 20:46:30.093117: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918dec00 of size 256 next 44\n",
      "2023-09-18 20:46:30.093125: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918ded00 of size 256 next 42\n",
      "2023-09-18 20:46:30.093133: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918dee00 of size 1024 next 40\n",
      "2023-09-18 20:46:30.093141: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918df200 of size 256 next 45\n",
      "2023-09-18 20:46:30.093150: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918df300 of size 256 next 46\n",
      "2023-09-18 20:46:30.093158: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918df400 of size 256 next 47\n",
      "2023-09-18 20:46:30.093166: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918df500 of size 256 next 48\n",
      "2023-09-18 20:46:30.093174: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918df600 of size 256 next 49\n",
      "2023-09-18 20:46:30.093182: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918df700 of size 256 next 50\n",
      "2023-09-18 20:46:30.093189: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918df800 of size 256 next 51\n",
      "2023-09-18 20:46:30.093198: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918df900 of size 256 next 56\n",
      "2023-09-18 20:46:30.093206: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918dfa00 of size 256 next 53\n",
      "2023-09-18 20:46:30.093214: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918dfb00 of size 256 next 54\n",
      "2023-09-18 20:46:30.093222: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918dfc00 of size 2048 next 55\n",
      "2023-09-18 20:46:30.093230: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918e0400 of size 256 next 52\n",
      "2023-09-18 20:46:30.093238: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918e0500 of size 256 next 58\n",
      "2023-09-18 20:46:30.093246: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918e0600 of size 256 next 60\n",
      "2023-09-18 20:46:30.093254: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2d918e0700 of size 512 next 59\n",
      "2023-09-18 20:46:30.093267: I tensorflow/tsl/framework/bfc_allocator.cc:1095] InUse at 7f2d918e0900 of size 1024 next 57\n",
      "2023-09-18 20:46:30.093276: I tensorflow/tsl/framework/bfc_allocator.cc:1095] Free  at 7f2d918e0d00 of size 230814464 next 18446744073709551615\n",
      "2023-09-18 20:46:30.093286: I tensorflow/tsl/framework/bfc_allocator.cc:1100]      Summary of in-use Chunks by size: \n",
      "2023-09-18 20:46:30.093300: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 50 Chunks of size 256 totalling 12.5KiB\n",
      "2023-09-18 20:46:30.093312: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 1024 totalling 3.0KiB\n",
      "2023-09-18 20:46:30.093322: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2023-09-18 20:46:30.093331: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 3 Chunks of size 2048 totalling 6.0KiB\n",
      "2023-09-18 20:46:30.093340: I tensorflow/tsl/framework/bfc_allocator.cc:1103] 2 Chunks of size 768006144 totalling 1.43GiB\n",
      "2023-09-18 20:46:30.093350: I tensorflow/tsl/framework/bfc_allocator.cc:1107] Sum Total of in-use chunks: 1.43GiB\n",
      "2023-09-18 20:46:30.093359: I tensorflow/tsl/framework/bfc_allocator.cc:1109] Total bytes in pool: 1766850560 memory_limit_: 1766850560 available bytes: 0 curr_region_allocation_bytes_: 3533701120\n",
      "2023-09-18 20:46:30.093378: I tensorflow/tsl/framework/bfc_allocator.cc:1114] Stats: \n",
      "Limit:                      1766850560\n",
      "InUse:                      1536035584\n",
      "MaxInUse:                   1536038144\n",
      "NumAllocs:                          86\n",
      "MaxAllocSize:                768006144\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-09-18 20:46:30.093394: W tensorflow/tsl/framework/bfc_allocator.cc:497] ***************************************************************************************_____________\n",
      "2023-09-18 20:46:30.093441: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at stateless_random_ops_v2.cc:64 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[250002,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer 'longformer' (type TFLongformerMainLayer).\n\n{{function_node __wrapped__StatelessTruncatedNormalV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[250002,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessTruncatedNormalV2] name: \n\nCall arguments received by layer 'longformer' (type TFLongformerMainLayer):\n  • input_ids=tf.Tensor(shape=(1, 2), dtype=int64)\n  • attention_mask=tf.Tensor(shape=(1, 2), dtype=int64)\n  • head_mask=None\n  • global_attention_mask=tf.Tensor(shape=(1, 2), dtype=int64)\n  • token_type_ids=tf.Tensor(shape=(1, 2), dtype=int32)\n  • position_ids=None\n  • inputs_embeds=None\n  • output_attentions=False\n  • output_hidden_states=False\n  • return_dict=True\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-base', num_labels=2)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m AutoModelForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mhyperonym/xlm-roberta-longformer-base-16384\u001b[39;49m\u001b[39m'\u001b[39;49m, from_tf\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, num_labels\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    565\u001b[0m     )\n\u001b[1;32m    566\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/transformers/modeling_utils.py:3153\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3150\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   3151\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_pytorch_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m load_tf2_checkpoint_in_pytorch_model\n\u001b[0;32m-> 3153\u001b[0m     model, loading_info \u001b[39m=\u001b[39m load_tf2_checkpoint_in_pytorch_model(\n\u001b[1;32m   3154\u001b[0m         model, resolved_archive_file, allow_missing_keys\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, output_loading_info\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m   3155\u001b[0m     )\n\u001b[1;32m   3156\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m   3157\u001b[0m     logger\u001b[39m.\u001b[39merror(\n\u001b[1;32m   3158\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLoading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Please see https://pytorch.org/ and https://www.tensorflow.org/install/ for installation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3160\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m instructions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3161\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/transformers/modeling_tf_pytorch_utils.py:449\u001b[0m, in \u001b[0;36mload_tf2_checkpoint_in_pytorch_model\u001b[0;34m(pt_model, tf_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info)\u001b[0m\n\u001b[1;32m    446\u001b[0m     tf_inputs \u001b[39m=\u001b[39m tf_model\u001b[39m.\u001b[39mdummy_inputs\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m tf_inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m     tf_model(tf_inputs, training\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)  \u001b[39m# Make sure model is built\u001b[39;00m\n\u001b[1;32m    451\u001b[0m load_tf_weights(tf_model, tf_checkpoint_path)\n\u001b[1;32m    453\u001b[0m \u001b[39mreturn\u001b[39;00m load_tf2_model_in_pytorch_model(\n\u001b[1;32m    454\u001b[0m     pt_model, tf_model, allow_missing_keys\u001b[39m=\u001b[39mallow_missing_keys, output_loading_info\u001b[39m=\u001b[39moutput_loading_info\n\u001b[1;32m    455\u001b[0m )\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    425\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 426\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/transformers/models/longformer/modeling_tf_longformer.py:2359\u001b[0m, in \u001b[0;36mTFLongformerForSequenceClassification.call\u001b[0;34m(self, input_ids, attention_mask, head_mask, token_type_ids, position_ids, global_attention_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001b[0m\n\u001b[1;32m   2348\u001b[0m     indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mpad(\n\u001b[1;32m   2349\u001b[0m         tensor\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mexpand_dims(tf\u001b[39m.\u001b[39mrange(shape_list(input_ids)[\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint64), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),\n\u001b[1;32m   2350\u001b[0m         paddings\u001b[39m=\u001b[39m[[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m], [\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]],\n\u001b[1;32m   2351\u001b[0m         constant_values\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m   2352\u001b[0m     )\n\u001b[1;32m   2353\u001b[0m     global_attention_mask \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtensor_scatter_nd_update(\n\u001b[1;32m   2354\u001b[0m         global_attention_mask,\n\u001b[1;32m   2355\u001b[0m         indices,\n\u001b[1;32m   2356\u001b[0m         updates,\n\u001b[1;32m   2357\u001b[0m     )\n\u001b[0;32m-> 2359\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlongformer(\n\u001b[1;32m   2360\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   2361\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   2362\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   2363\u001b[0m     global_attention_mask\u001b[39m=\u001b[39;49mglobal_attention_mask,\n\u001b[1;32m   2364\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   2365\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   2366\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   2367\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2368\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2369\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   2370\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   2371\u001b[0m )\n\u001b[1;32m   2372\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   2373\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:426\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    425\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 426\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/transformers/models/longformer/modeling_tf_longformer.py:1763\u001b[0m, in \u001b[0;36mTFLongformerMainLayer.call\u001b[0;34m(self, input_ids, attention_mask, head_mask, global_attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[39m# Since attention_mask is 1.0 for positions we want to attend locally and 0.0 for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[39m# masked and global attn positions, this operation will create a tensor which is 0.0 for\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[39m# positions we want to attend and -10000.0 for masked positions.\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m \u001b[39m# Since we are adding it to the raw scores before the softmax, this is\u001b[39;00m\n\u001b[1;32m   1761\u001b[0m \u001b[39m# effectively the same as removing these entirely.\u001b[39;00m\n\u001b[1;32m   1762\u001b[0m extended_attention_mask \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mabs(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m extended_attention_mask), tf\u001b[39m.\u001b[39mdtypes\u001b[39m.\u001b[39mfloat32) \u001b[39m*\u001b[39m \u001b[39m-\u001b[39m\u001b[39m10000.0\u001b[39m\n\u001b[0;32m-> 1763\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m   1764\u001b[0m     input_ids,\n\u001b[1;32m   1765\u001b[0m     position_ids,\n\u001b[1;32m   1766\u001b[0m     token_type_ids,\n\u001b[1;32m   1767\u001b[0m     inputs_embeds,\n\u001b[1;32m   1768\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1769\u001b[0m )\n\u001b[1;32m   1770\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m   1771\u001b[0m     embedding_output,\n\u001b[1;32m   1772\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1781\u001b[0m     training\u001b[39m=\u001b[39mtraining,\n\u001b[1;32m   1782\u001b[0m )\n\u001b[1;32m   1783\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/transformers/models/longformer/modeling_tf_longformer.py:489\u001b[0m, in \u001b[0;36mTFLongformerEmbeddings.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild\u001b[39m(\u001b[39mself\u001b[39m, input_shape: tf\u001b[39m.\u001b[39mTensorShape):\n\u001b[1;32m    488\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mword_embeddings\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 489\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_weight(\n\u001b[1;32m    490\u001b[0m             name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mweight\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    491\u001b[0m             shape\u001b[39m=\u001b[39;49m[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mvocab_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_size],\n\u001b[1;32m    492\u001b[0m             initializer\u001b[39m=\u001b[39;49mget_initializer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minitializer_range),\n\u001b[1;32m    493\u001b[0m         )\n\u001b[1;32m    495\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mname_scope(\u001b[39m\"\u001b[39m\u001b[39mtoken_type_embeddings\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    496\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken_type_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_weight(\n\u001b[1;32m    497\u001b[0m             name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    498\u001b[0m             shape\u001b[39m=\u001b[39m[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mtype_vocab_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size],\n\u001b[1;32m    499\u001b[0m             initializer\u001b[39m=\u001b[39mget_initializer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minitializer_range),\n\u001b[1;32m    500\u001b[0m         )\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer 'longformer' (type TFLongformerMainLayer).\n\n{{function_node __wrapped__StatelessTruncatedNormalV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[250002,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessTruncatedNormalV2] name: \n\nCall arguments received by layer 'longformer' (type TFLongformerMainLayer):\n  • input_ids=tf.Tensor(shape=(1, 2), dtype=int64)\n  • attention_mask=tf.Tensor(shape=(1, 2), dtype=int64)\n  • head_mask=None\n  • global_attention_mask=tf.Tensor(shape=(1, 2), dtype=int64)\n  • token_type_ids=tf.Tensor(shape=(1, 2), dtype=int32)\n  • position_ids=None\n  • inputs_embeds=None\n  • output_attentions=False\n  • output_hidden_states=False\n  • return_dict=True\n  • training=False"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-base', num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('hyperonym/xlm-roberta-longformer-base-16384', from_tf=True, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForSequenceClassification(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classification_head): BartClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = AdamW(params=model.parameters(), lr=5e-6)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=TRAIN_STEPS\n",
    ")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataloader, \n",
    "                epochs, \n",
    "                train_steps, \n",
    "                optimizer, \n",
    "                lr_scheduler):\n",
    "\n",
    "    progress_bar = tqdm(range(train_steps))\n",
    "    losses = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_dataloader:\n",
    "            # get batch data\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            # print(batch)\n",
    "\n",
    "            # get model output\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            # get loss and backprop\n",
    "            loss = outputs.loss\n",
    "            losses.append(loss)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # update progress\n",
    "            progress_bar.update(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1184508 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[[    0, 37635, 28500,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[    0, 34928, 28500,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[    0, 37635, 28500,  ...,     1,     1,     1]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    0, 37635, 28500,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[    0, 37635, 28500,  ...,     1,     1,     1]],\n",
      "\n",
      "        [[    0, 37635, 28500,  ...,     1,     1,     1]]], device='cuda:0')}\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 1.95 GiB total capacity; 543.10 MiB already allocated; 1.32 GiB free; 590.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_model(train_dl,\n\u001b[1;32m      2\u001b[0m             EPOCHS,\n\u001b[1;32m      3\u001b[0m             TRAIN_STEPS,\n\u001b[1;32m      4\u001b[0m             optimizer,\n\u001b[1;32m      5\u001b[0m             lr_scheduler)\n",
      "Cell \u001b[0;32mIn[32], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_dataloader, epochs, train_steps, optimizer, lr_scheduler)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(batch)\n\u001b[1;32m     17\u001b[0m \u001b[39m# get model output\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m     20\u001b[0m \u001b[39m# get loss and backprop\u001b[39;00m\n\u001b[1;32m     21\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:1538\u001b[0m, in \u001b[0;36mBartForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1534\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m   1535\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing input embeddings is currently not supported for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1536\u001b[0m     )\n\u001b[0;32m-> 1538\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1539\u001b[0m     input_ids,\n\u001b[1;32m   1540\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1541\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   1542\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   1543\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1544\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   1545\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   1546\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   1547\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1548\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   1549\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1550\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1551\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1552\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1553\u001b[0m )\n\u001b[1;32m   1554\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]  \u001b[39m# last hidden state\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m eos_mask \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39meq(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39meos_token_id)\u001b[39m.\u001b[39mto(hidden_states\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:1256\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1253\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1255\u001b[0m \u001b[39mif\u001b[39;00m encoder_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1256\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1257\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1258\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1259\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1260\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1261\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1262\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1263\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1264\u001b[0m     )\n\u001b[1;32m   1265\u001b[0m \u001b[39m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m \u001b[39melif\u001b[39;00m return_dict \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:818\u001b[0m, in \u001b[0;36mBartEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    817\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 818\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_tokens(input_ids) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_scale\n\u001b[1;32m    820\u001b[0m embed_pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_positions(\u001b[39minput\u001b[39m)\n\u001b[1;32m    821\u001b[0m embed_pos \u001b[39m=\u001b[39m embed_pos\u001b[39m.\u001b[39mto(inputs_embeds\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/projects/sih/ml_components/env/lib/python3.11/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.50 GiB (GPU 0; 1.95 GiB total capacity; 543.10 MiB already allocated; 1.32 GiB free; 590.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "train_model(train_dl,\n",
    "            EPOCHS,\n",
    "            TRAIN_STEPS,\n",
    "            optimizer,\n",
    "            lr_scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
